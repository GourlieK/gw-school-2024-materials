{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45047af-c68b-4651-adc6-e8ebafd880f1",
   "metadata": {},
   "source": [
    "# Continuous Waves Continued\n",
    "\n",
    "In yesterday's tutorial, we may have touched on one of most important pieces of information we can obtain from an MCMC analysis: the **sky location of the CW source**. If we know where the GW signal came from on the sky, we can then look for host galaxies in that region, or potential electromagnetic signatures of supermassive binaries, in the hopes that we can connect the two in a multi-messenger detection.\n",
    "\n",
    "Parts of this tutorial are taken from https://rtd.igwn.org/projects/userguide/en/v15/tutorial/skymaps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb27f0f-e7be-459f-8151-20dda8a9fdc8",
   "metadata": {},
   "source": [
    "# 0. Package installation & imports\n",
    "Follow the instructions in the link below to install `ligo.skymap` in your conda environment.\n",
    "\n",
    "https://lscsoft.docs.ligo.org/ligo.skymap/quickstart/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf491c-d4d8-4ad9-8305-b1b6782ae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import csv\n",
    "from enterprise.pulsar import Pulsar\n",
    "from enterprise import constants as const\n",
    "import healpy as hp\n",
    "import ligo.skymap\n",
    "import ligo.skymap.plot\n",
    "import ligo.skymap.io\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e772e-96c3-43de-b4d4-f6695cd5f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory will be the same as this notebook\n",
    "basedir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655f06e-acb8-4bce-8c3a-a0bc7ba4ccce",
   "metadata": {},
   "source": [
    "# 1. Review: signal localization + tools\n",
    "\n",
    "How much information does a Bayesian analysis give us about the CW's sky location?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03c891-8fc7-4169-94c8-6b573813f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example pulsars from yesterday\n",
    "with open(basedir+'example_dataset.pkl', 'rb') as expkl:\n",
    "    ex_psrs = pickle.load(expkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4e1da-838a-47aa-b92c-176a650f59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example chain file from yesterday\n",
    "ex_file = basedir+'example_chain.h5'\n",
    "with h5py.File(ex_file, 'r') as ff:\n",
    "    chain = ff['samples_cold'][0,::,:]\n",
    "    print('Number of samples:', chain.shape)\n",
    "    par_names = [x.decode('UTF-8') for x in list(ff['par_names'])]\n",
    "    log_likelihood = ff['log_likelihood'][:,::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f4e90-7499-472b-89da-7f0221e4b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need our cos_gwtheta samples and gwphi samples\n",
    "costheta_samples = chain[:,0]\n",
    "phi_samples = chain[:,2]\n",
    "sky_samples = np.stack([costheta_samples,phi_samples], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fbbbb5-1026-45c3-b974-3053ad61f852",
   "metadata": {},
   "source": [
    "First, let's see how the injected values compare to the signal localization using a corner plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35d7b2-cdbe-4e36-8b93-09d474298102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# injected values\n",
    "ex_ra = (15*(13 + 0/60 + 8.09/3600))\n",
    "ex_dec = 27 + 58/60 + 37.2/3600\n",
    "ex_gwphi = ex_ra*np.pi/180\n",
    "ex_gwtheta = np.pi/2 - ex_dec*np.pi/180\n",
    "\n",
    "# corner plot\n",
    "burn = int(len(chain)/4)\n",
    "corner.corner(sky_samples[burn:,0:2], levels=[0.68,0.95],\n",
    "              truths=[np.cos(ex_gwtheta),ex_gwphi], truth_color='k', labels=['cos_gwtheta','gwphi'], color='C0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd3cdf-25a5-492a-87e0-1ef4ac160881",
   "metadata": {},
   "source": [
    "### Skymap visualization with `healpy`\n",
    "\n",
    "Looks like the GW signal is recovered fairly well! Now let's see where this region lies on the sky using `healpy`. This software package uses the **H**ierarchical **E**qual **A**rea iso**L**atitude **Pix**elization (HEALPix) scheme, which basically means that it subdivides the sky into pixels that each cover the same surface area. See this link for more info and some nice graphics to illustrate this: https://healpix.jpl.nasa.gov/\n",
    "\n",
    "First, we need to transform our samples into a HEALPix map, which we'll do using the function below. The function takes in our sky location samples as well as a parameter called $N_{\\rm{side}}$, which is the resolution of our skymap. The lowest resolution is $N_{\\rm{side}} = 1$, which will split the sky into $12N_{\\rm{side}}^2$ equal-area pixels. $N_{\\rm{side}}$ values must be a power of 2, i.e., 1, 2, 4, 8, 16, and so on. We'll start with $N_{\\rm{side}}=8$, which is a total of 768 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710a684-f35f-474b-9d32-783f688cf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert samples into a HEALPix map of number counts\n",
    "\n",
    "def post2hpx(theta, phi, nside=8):\n",
    "    \n",
    "    # converts nside resolution to total number of pixels\n",
    "    # (this is 768 for nside=8)\n",
    "    npix = hp.nside2npix(nside) \n",
    "    \n",
    "    # based on a given nside resolution,\n",
    "    # takes sky samples and assigns a pixel index to each sample\n",
    "    # (for nside=8 each sample will be assigned a value of 0 through 767 based on where it lies on the sky)\n",
    "    indices = hp.ang2pix(nside, theta, phi)\n",
    "\n",
    "    # determine how many samples correspond to each pixel\n",
    "    # this will give us the density of our samples across the sky\n",
    "    idx, counts = np.unique(indices, return_counts=True)\n",
    "    \n",
    "    # make a blank skymap and then fill it with the counts, or density, of samples\n",
    "    hpx_map = np.zeros(npix, dtype=int)\n",
    "    hpx_map[idx] = counts\n",
    "    \n",
    "    return hpx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf1319-2c68-4f07-9566-3198d4133e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the healpy map\n",
    "hpx_map = post2hpx(np.arccos(costheta_samples), phi_samples, nside=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0560dd-b873-481e-a010-1ab19f7b52e4",
   "metadata": {},
   "source": [
    "**Take some time to understand what this function is doing.** Print out each step if you need to! When you feel comfortable with the samples-to-skymap conversion, run the cell below to see the skymap. We're plotting using a mollweide (equal-area) projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecf5e5-75f0-4912-96f8-db3fd1ab9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're using a mollweide projection map - this is pretty standard\n",
    "hp.mollview(hpx_map, rot=180, hold=True, title='')\n",
    "\n",
    "# plot pulsar positions\n",
    "for i, psr in enumerate(ex_psrs):\n",
    "    hp.visufunc.projscatter(psr.theta, psr.phi, marker='*', s=70, color='lightblue')\n",
    "\n",
    "# plot GW source location\n",
    "hp.visufunc.projscatter(ex_gwtheta, ex_gwphi, marker='x', s=50, color='r')\n",
    "\n",
    "# plot axis labels by hand (healpy is weird about this)\n",
    "for i in range(2,24,2):\n",
    "    text = hp.projtext(i*180/12+3, 4, str(i)+'h', lonlat=True, coord='G', fontsize='medium', zorder=1, color='w')\n",
    "for i in range(-75,0,15):\n",
    "    text = hp.projtext(180, i, str(i)+'$^\\circ$', lonlat=True, coord='G', fontsize='medium', zorder=10, color='w')\n",
    "for i in range(15,90,15):\n",
    "    text = hp.projtext(180, i, str(i)+'$^\\circ$', lonlat=True, coord='G', fontsize='medium', zorder=10, color='w')\n",
    "\n",
    "# plot grid lines\n",
    "hp.graticule(15, 30, color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfe660-e523-450e-ab74-9eaeb3d806ed",
   "metadata": {},
   "source": [
    "The colored pixels correspond to our posterior samples, with yellow pixels having more samples. See the colorbar for the number of samples lying in each pixel. **Feel free to play around with the resolution of the map by changing the $N_{\\rm{side}}$ value** (with higher $N_{\\rm{side}}$ being higher resolution). For reference, we've also plotted this simulated array's pulsars in light blue, and the red X marker is the true injected sky location.\n",
    "\n",
    "We can also plot the same map but instead with normalized counts, i.e., the percentage of points falling within each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f61e71-355f-4a92-91b3-42074a2ffda6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# healpix map with normalized counts, i.e., percentage of points falling in each pixel\n",
    "norm_map = hpx_map / np.sum(hpx_map)\n",
    "print(norm_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919412ea-2b60-4077-b7da-4dc97f410a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mollweide projection map\n",
    "hp.mollview(norm_map, rot=180, hold=True, title='')\n",
    "\n",
    "# plot pulsar positions\n",
    "for i, psr in enumerate(ex_psrs):\n",
    "    hp.visufunc.projscatter(psr.theta, psr.phi, marker='*', s=70, color='lightblue')\n",
    "\n",
    "# plot GW source location\n",
    "hp.visufunc.projscatter(ex_gwtheta, ex_gwphi, marker='x', s=50, color='r')\n",
    "\n",
    "# plot axis labels by hand\n",
    "for i in range(2,24,2):\n",
    "    text = hp.projtext(i*180/12+3, 4, str(i)+'h', lonlat=True, coord='G', fontsize='medium', zorder=1, color='w')\n",
    "for i in range(-75,0,15):\n",
    "    text = hp.projtext(180, i, str(i)+'$^\\circ$', lonlat=True, coord='G', fontsize='medium', zorder=10, color='w')\n",
    "for i in range(15,90,15):\n",
    "    text = hp.projtext(180, i, str(i)+'$^\\circ$', lonlat=True, coord='G', fontsize='medium', zorder=10, color='w')\n",
    "\n",
    "# plot grid lines\n",
    "hp.graticule(15, 30, color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a124e15c-f482-42ca-a976-39145b51f760",
   "metadata": {},
   "source": [
    "So according to the colorbar, our densest pixel in yellow contains about 15% of our posterior samples, with darker pixels being less dense in posterior samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd5c7b-8983-4668-b3a1-ef2b5b5b87ea",
   "metadata": {},
   "source": [
    "### Other `healpy` tools\n",
    "Let's break down the HEALPix map a bit more. Each entry in our `hpx_map` array represents the probability contained within a quadrilateral pixel, and each pixel's position on the sky is uniquely specified by the index in the array and the array’s length. `healpy` has some handy tools you can use for conversions between pixels and sky positions, finding the number of pixels for a given resolution, finding the angular size of one pixel for a given resolution, etc. The cells below include a couple simple questions you can answer with these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911b7b3-b3ba-4c84-84c2-f3725a025ea3",
   "metadata": {},
   "source": [
    "**What is the right ascension and declination of pixel number 123, for an $N_{\\rm{side}}=8$ resolution, i.e., a 768-pixel map?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e0d24-5ca2-40da-8691-81294e397e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we call hp.pix2ang to get the spherical polar coordinates (theta,phi) in radians\n",
    "nside = 8 #resolution\n",
    "ipix = 123 #pixel index\n",
    "theta, phi = hp.pix2ang(nside, ipix)\n",
    "\n",
    "# then use np.rad2deg to convert these to right ascension and declination in degrees\n",
    "ra = np.rad2deg(phi)\n",
    "dec = np.rad2deg(np.pi/2 - theta)\n",
    "print(ra, dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaeebad-583b-45ba-8081-18c8276c9c35",
   "metadata": {},
   "source": [
    "**In the reverse scenario, which pixel number contains the point RA=194.95 deg, Dec=27.98 deg for an $N_{\\rm{side}}=8$ resolution?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189bf11-66de-43e6-9601-6dc749f36da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from RA and dec to phi and theta\n",
    "ra = 194.95\n",
    "dec = 27.98\n",
    "theta = np.pi/2 - np.deg2rad(dec)\n",
    "phi = np.deg2rad(ra)\n",
    "\n",
    "# use hp.ang2pix to convert coords to pixel index\n",
    "ipix = hp.ang2pix(nside, theta, phi)\n",
    "print(ipix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7805c-97e7-44fa-8d34-9384766ccaf8",
   "metadata": {},
   "source": [
    "**What is the area of one pixel for an $N_{\\rm{side}}=8$ resolution?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da6c1a6-1f77-490c-94f1-940e5a0825b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hp.nside2pixarea for this\n",
    "nside_area = hp.nside2pixarea(nside, degrees=True)\n",
    "print(nside_area, 'sq deg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ca7e8-e651-41ce-9a9c-4b82bc0579a4",
   "metadata": {},
   "source": [
    "# 2. Credible levels + localization area\n",
    "### Credible level map\n",
    "In addition to creating a density map from our posterior samples, we can also construct a map that gives the credible level of each pixel.  A credible interval is just the range containing a particular percentage of probable values. For example, the 95% credible interval is just the central portion of a posterior distribution containing 95% of the values. Credible intervals are the Bayesian analog to confidence intervals in frequentist statistics.\n",
    "\n",
    "We can use the following algorithm to make our credible level map (which gives the credible level of each pixel):\n",
    "<ol>\n",
    "    1. Sort the pixels by descending probability density <br>\n",
    "    2. Cumulatively sum the pixels’ probability <br>\n",
    "    3. Return the pixels to their original order\n",
    "</ol>\n",
    "\n",
    "In Python, we can use this simple recipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65ae1d-ad6a-47c3-8082-2685436f9544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort the pixels by ascending probability density,\n",
    "# then flip the array to be descending probability density\n",
    "sort = np.flipud(np.argsort(norm_map))\n",
    "\n",
    "# cumulatively sum the pixels’ probability, in this new sorted order\n",
    "sorted_credible_levels = np.cumsum(norm_map[sort])\n",
    "\n",
    "# return pixels to their original order,\n",
    "# populating an empty array with the credible levels assigned to each pixel\n",
    "credible_levels = np.empty_like(sorted_credible_levels)\n",
    "credible_levels[sort] = sorted_credible_levels\n",
    "print(credible_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f2333-8cb4-4b31-9a53-b0aa1cbfee43",
   "metadata": {},
   "source": [
    "**Note** that the values in the resulting credible level map vary inversely with probability density: the most probable pixel is assigned to the credible level 0.0, and the least likely pixel is assigned the credible level 1.0.\n",
    "\n",
    "When examining localization areas from GW analyses, it's pretty standard to look at the 90% credible region, i.e., the area in which 90% of our posterior samples fall. **Let's try out some different pixel indices and check if they fall within the 90% credible region.** To do this, we test if the value of the credible level map is less than or equal to 0.9 at that pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6241910-6b76-4411-b0a0-1dce22701c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take pixel index 123 from our earlier example\n",
    "ipix = 123\n",
    "print(credible_levels[ipix])\n",
    "print(credible_levels[ipix] <= 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb498b-e969-4f76-9b96-8a096a94f57f",
   "metadata": {},
   "source": [
    "**Rerun the cell above, trying different choices of pixel indices and credible levels.** Keep in mind we're using a 768-pixel map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91125c71-f35d-4a66-b52f-0c895abd0530",
   "metadata": {},
   "source": [
    "### Most probable sky location\n",
    "Now let’s find the highest probability pixel. **Where is the highest probability pixel on the sky? What is the probability density per square degree at that position?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94a4b7-fda6-4c81-a612-88b9df168ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of the maximum probability pixel (this is our density map)\n",
    "ipix_max = np.argmax(___)\n",
    "\n",
    "# use the appropriate function to convert the resolution and index to the sky position\n",
    "nside = 8\n",
    "theta, phi = hp.___(___, ___)\n",
    "ra = np.rad2deg(phi)\n",
    "dec = np.rad2deg(np.pi/2 - theta)\n",
    "print('Highest probability pixel on the sky:', ra, dec)\n",
    "\n",
    "# probability density per square degree at that pixel\n",
    "# the function hp.nside2pixarea converts the resolution to the area of one pixel\n",
    "pd = ___[___]\n",
    "pd_per_deg = pd / hp.___(nside, degrees=True)\n",
    "print('Probability density per square degree at highest probability pixel:', pd_per_deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8e5c7-66cf-499f-a163-48232bf3c356",
   "metadata": {},
   "source": [
    "### Finding the area of a given credible region\n",
    "With our credible level map, it’s straightforward to now compute the 90% credible area. We do this simply by counting the number of pixels inside the 90% credible region and multiplying by the area per pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dc15e-a546-4857-b10b-641c7b71f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a credible level of 90%\n",
    "# finding all pixels falling within this credible level\n",
    "w90 = np.where(credible_levels <= 0.9)[0]\n",
    "\n",
    "# this will give us the true credible level (the maximum credible level among those that are <= 90%)\n",
    "true_cl = max(credible_levels[w90])\n",
    "print('Credible level:', true_cl)\n",
    "\n",
    "# area calculation\n",
    "area = np.sum(credible_levels <= 0.9) * hp.nside2pixarea(nside, degrees=True)\n",
    "print('Area:', area, 'sq deg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3f033-ad75-48ce-831f-150439d3f9ae",
   "metadata": {},
   "source": [
    "**In the space below, try a range of different credible levels and calculate the localization area for each. You can also try plotting the resulting curve.** Note: you might encounter an error that looks like this: `ValueError: max() arg is an empty sequence.` This means that the credible level you chose is too small, i.e., there are no pixels that fall within that credible level. In this example, the densest pixel holds 15% of the posterior samples, so try again with a value higher than 0.15!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08dbce-3147-4271-abac-cde3893bf270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space to try a range of credible levels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1de92f-b0b2-4a12-aab4-5cd5b3568267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the areas as a function of credible level\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab204bf3-4840-44a0-95ce-c348fdd1d4d2",
   "metadata": {},
   "source": [
    "### Skymap plotting with `ligo.skymap`\n",
    "\n",
    "https://lscsoft.docs.ligo.org/ligo.skymap/index.html <br>\n",
    "https://lscsoft.docs.ligo.org/ligo.skymap/plot/allsky.html#module-ligo.skymap.plot.allsky\n",
    "\n",
    "Let's get a better visualization of this by looking at these different credible levels and areas on a skymap. Here we'll switch to `ligo.skymap` instead of `healpy` plotting $-$ it's the skymap plotting tool developed and used by LIGO, and it has a lot of cool functionalities we'll get practice with throughout the rest of the notebook!\n",
    "\n",
    "First we'll plot the 90% credible area we calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe335a-7f72-4668-ad92-233b9ac67066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the typical skymap projection and takes care of all coordinate transforms!\n",
    "plt.figure()\n",
    "ax = plt.axes(projection='astro hours mollweide') \n",
    "\n",
    "# this is our pixelated 2D histogram map\n",
    "ax.imshow_hpx(norm_map, cmap='Oranges')\n",
    "\n",
    "# need the line below to set the 90% contour level - finding the index of the true credible level\n",
    "cut90 = np.argmax(credible_levels[w90])\n",
    "\n",
    "# plot the 90% contour\n",
    "ax.contour_hpx(norm_map, cmap=mpl.colors.ListedColormap(['k']), linewidths=1.5, levels=[norm_map[w90][cut90]])\n",
    "\n",
    "# plot the pulsars in this simulated array\n",
    "psr_ra = [p.phi*180/np.pi for p in ex_psrs]\n",
    "psr_dec = [(np.pi/2-p.theta)*180/np.pi for p in ex_psrs]\n",
    "ax.plot(psr_ra, psr_dec,\n",
    "        marker='*', color='gold', markeredgecolor='k', ls='', alpha=0.75, ms=10, \n",
    "        transform=ax.get_transform('world'))\n",
    "\n",
    "# plot the injected GW location\n",
    "gw_ra = 15*(13 + 0/60 + 8.09/3600)\n",
    "gw_dec = 27 + 58/60 + 37.2/3600\n",
    "gwphi = gw_ra*np.pi/180\n",
    "gwtheta = np.pi/2 - gw_dec*np.pi/180\n",
    "ax.plot(gw_ra, gw_dec,\n",
    "        marker='D', color='w', markeredgecolor='k', ls='', ms=6, \n",
    "        transform=ax.get_transform('world'))\n",
    "\n",
    "# other plotting params\n",
    "ax.grid(alpha=0.3, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfaa51-c819-4823-bc17-3c30b63a05b0",
   "metadata": {},
   "source": [
    "This skymap is a noticeably a little smoothed out compared to the one we plotted with `healpy`, which is something `ligo.skymap` does internally. But now we've got a nice black contour boundary, which makes it really easy to see the area of the sky encompassed in the 90% credible region. **Use the space below to plot some other credible areas you found earlier.** Make sure you find the levels value needed for the `ax.contour_hpx` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb920d6-7672-487a-b871-c522480479de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space to plot other credible regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d47d60-f8dc-4f92-8d04-ae42fbf98ea2",
   "metadata": {},
   "source": [
    "# 3. Looking for host galaxies\n",
    "\n",
    "Now that we've got the tools to do skymap plotting, let's dive into some of those multi-messenger questions we touched on earlier. The localization region we plotted spans an area of ~ 860 deg$^2$! That's a pretty large chunk of the sky that we need to search through to find the host galaxy of the SMBHB emitting the GW signal... how do we narrow down where the GW signal is coming from?\n",
    "\n",
    "Identifying the correct host galaxy is still an open challenge when it comes to PTA multi-messenger studies. We heard in the morning session about a variety of possible EM signatures that SMBHBs may generate. Maybe we take a look into the systems lying within this localization region and check to see if any of them produce the kinds of EM signatures we think SMBHBs should make. But the **ambiguity in EM signatures** makes it difficult for us to be completely sure of what we're looking for! For example, we don't have sure-fire EM evidence of SMBHBs in the way that LIGO neutron star binaries do. And aside from that, the ~ 860 deg$^2$ area we saw above is not unusual for a PTA localization region. PTAs have pretty **poor localization capability** $-$ for the first GW signals that PTAs will localize, we expect to see areas on the order of ~ 100-1000 deg$^2$.\n",
    "\n",
    "But just how many potential host galaxies lie in regions of this size? **Let's take a look using NANOGrav's catalog of massive galaxies in the local universe.** This catalog was assembled from the 2MASS Redshift Survey (done in infrared bands), and it contains sky coordinates, distances, and SMBH masses for 43,532 galaxies out to redshift z ∼ 0.05. The catalog is 97.6% complete to ∼ 300 Mpc and apparent $K$-band magnitudes $m_K$ ≤ 11.75, and it extends out to ∼ 500−700 Mpc for the most massive galaxies that will be prime targets for PTAs.\n",
    "\n",
    "For more on the 2MASS Redshift Survey: https://arxiv.org/abs/1108.0669 <br>\n",
    "For more on the NANOGrav galaxy catalog: https://arxiv.org/abs/2101.02716"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25648db5-5cb3-43ef-95a1-e6f65d90c361",
   "metadata": {},
   "source": [
    "### Galaxy catalog\n",
    "**Run the cell below to read in the catalog and examine its contents.** A few important details:\n",
    "<il>\n",
    "- The galaxy name is given in the format (RA, dec) = (hhmmss.ss, +/- ddmmss.s) where h=hours, d=degrees, m=minutes, s=seconds.\n",
    "- In some cases, there are two different estimates of the SMBH mass, which comes from the $M_{\\rm{BH}} - M_{\\rm{bulge}}$ relationship. One estimate uses the assumption $f_{\\rm{bulge}}=1$ and the other uses $f_{\\rm{bulge}}=0.31$.\n",
    "- Uncertainty on the SMBH mass is given in $\\rm{M}_\\odot$.\n",
    "- The distance and mass flags indicate the methods used to calculate these quantities.\n",
    "</il>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0113a-d62e-4aee-9b3a-77a6573350b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine galaxy catalog's contents\n",
    "with open(basedir+'gxy_catalog.txt') as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dfbaba-98be-46ca-9444-0dd932286b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in galaxy catalog\n",
    "catalog = csv.reader(open(basedir+'gxy_catalog.txt'), delimiter=\"\\t\")\n",
    "gxy_data = list(catalog)[20:-1]\n",
    "gxy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd37202-9375-4e30-a7f1-15701606aa76",
   "metadata": {},
   "source": [
    "We now have the data separated into an array for each galaxy, but the different quantities for each galaxy are still given as strings. Let's make separate arrays for each column of the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfae52a-bb59-4293-9fa6-3828c77e964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns for galaxy name (ID), distance, and BH mass (f_bulge=1)\n",
    "\n",
    "IDs = []\n",
    "dists = []\n",
    "BHmasses = []\n",
    "for line in gxy_data:\n",
    "    new_line = line[0].split(' ')\n",
    "    new_line = [x for x in new_line if len(x)>0]\n",
    "    IDs.append(new_line[0])\n",
    "    dists.append(float(new_line[1]))\n",
    "    BHmasses.append(float(new_line[2]))\n",
    "\n",
    "# turning these into numpy arrays will be more convenient later on\n",
    "IDs = np.array(IDs)\n",
    "dists = np.array(dists)\n",
    "BHmasses = np.array(BHmasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db90c1-1447-4b16-aa0f-e0d568ecd3bc",
   "metadata": {},
   "source": [
    "### Host properties\n",
    "\n",
    "Let's get some familiarity with these galaxies' properties. Since we've just done a ton of skymap plotting, let's plot these galaxies on a skymap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a3fd6-3b38-4efe-a836-e30147d17a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use astropy to take care of the coordinate conversion - this might take a few seconds\n",
    "\n",
    "gxy_ra = []\n",
    "gxy_dec = []\n",
    "\n",
    "for gxyID in IDs:\n",
    "    \n",
    "    # split up the string according to the RA and Dec\n",
    "    ra = gxyID[0:2]+'h'+gxyID[2:4]+'m'+gxyID[4:6]+'.'+gxyID[6:8]+'s'\n",
    "    dec = gxyID[8:11]+'d'+gxyID[11:13]+'m'+gxyID[13:15]+'.'+gxyID[15]+'s'\n",
    "    \n",
    "    # make a SkyCoord object using astropy\n",
    "    # 'icrs' refers to the coordinate system/reference frame\n",
    "    coord = SkyCoord(ra, dec, frame='icrs')\n",
    "    \n",
    "    # grab RA and Dec values in units of degrees\n",
    "    gxy_ra.append(coord.ra.deg)\n",
    "    gxy_dec.append(coord.dec.deg)\n",
    "\n",
    "gxy_ra = np.array(gxy_ra)\n",
    "gxy_dec = np.array(gxy_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e19fc-7e7b-47a4-8941-d63184162955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all galaxies on a skymap\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "ax = plt.axes(projection='astro hours mollweide')\n",
    "ax.plot(gxy_ra, gxy_dec,\n",
    "        marker='o', color='dodgerblue', markeredgecolor='k', ls='', alpha=0.5, ms=3,\n",
    "        transform=ax.get_transform('world'))\n",
    "ax.grid(alpha=0.3, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6111a5b-1b04-4733-843e-d551e8471718",
   "metadata": {},
   "source": [
    "You'll notice there's an empty U-shaped band across the sky where we don't have many galaxies $-$ this is the plane of the Milky Way. It's difficult to see through our own galaxy out to the distances where other galaxies are, so the catalog doesn't really contain any galaxies in this region.\n",
    "\n",
    "**Let's take a look now at the distribution of distances and BH masses.** Note: there are a few quasars in this catalog, which have negative BH masses to differentiate them from the other galaxies. Set the x-limit accordingly on your BH mass plot to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a45a0-182c-4c43-b91d-8410a6a13c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44e94d-0a09-4f8f-968b-becbe91fdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of BH masses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787bd82-5f59-459d-bb36-e823ae057561",
   "metadata": {},
   "source": [
    "### How many potential hosts are we dealing with?\n",
    "\n",
    "So exactly how many potential host galaxies lie in our 90% credible area of ~ 860 deg$^2$? We'll take a look using the plotting function below, which does much of what we went through above. The function takes as inputs an array of host galaxy RA and dec values, a normalized HEALPix skymap, and a credible level (set to the standard 90% by default). Then, as we did above, it generates a credible level map, calculates the true credible level, the localization area, and plots the area on a skymap. Saving the contour path, it will then count up how many hosts lie within the localization area using a handy `matplotlib` function and plot those on the map as well. There's also an option to plot a zoom-in panel alongside the skymap to see the localization region more clearly! **At the end, it will return a mask array with the same length as your host galaxy coordinate arrays, with 0 for those that fall within the localization region, and 1 for those that lie outside of the localization region.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646810af-3933-4827-86c0-2137394cc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hosts(host_ra, host_dec, skymap, cl=0.9, zoom=False, second_mask=None):\n",
    "\n",
    "    '''\n",
    "    host_ra: array of host galaxy right ascension coordinates\n",
    "    host_dec: array of host galaxy declination coordinates\n",
    "    skymap: normalized healpix map\n",
    "    cl: credible level\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # first makes a credible level map from the normalized map\n",
    "    sort = np.flipud(np.argsort(skymap))\n",
    "    sorted_credible_levels = np.cumsum(skymap[sort])\n",
    "    credible_levels = np.empty_like(sorted_credible_levels)\n",
    "    credible_levels[sort] = sorted_credible_levels\n",
    "\n",
    "    # find the true credible level and area\n",
    "    w_cl = np.where(credible_levels <= cl)[0]\n",
    "    cut_cl = np.argmax(credible_levels[w_cl])\n",
    "    true_cl = credible_levels[w_cl][cut_cl]\n",
    "    print('Credible level:', true_cl)\n",
    "    area = np.sum(credible_levels <= cl) * hp.nside2pixarea(nside, degrees=True)\n",
    "    print('Area:', area, 'sq deg')\n",
    "\n",
    "    # plotting credible region\n",
    "    ax = plt.subplot(projection='astro hours mollweide')\n",
    "    ax.imshow_hpx(skymap, cmap='Oranges', smooth=None)\n",
    "    skyContour = ax.contour_hpx(skymap, cmap=mpl.colors.ListedColormap(['k']), levels=[skymap[w_cl][cut_cl]])\n",
    "\n",
    "    # need to transform RA, dec of host galaxies to the axis coordinates\n",
    "    transform_obj = ax.get_transform('world') + ax.transData.inverted()\n",
    "    gxy_pos = np.stack([host_ra, host_dec], axis=1)\n",
    "    gxy_pos_transform = transform_obj.transform(gxy_pos)\n",
    "\n",
    "    # using the boundary of the contour to find the number of galaxies within the credible region\n",
    "    # start with an array of 1's --> this means all galaxies will be masked out, i.e., not visible on the map\n",
    "    mask = np.ones(len(host_ra), dtype=bool)\n",
    "    \n",
    "    # for each contour path and each galaxy, check if the galaxy lies within the contour\n",
    "    for path in skyContour.collections[0].get_paths():\n",
    "        for j in range(len(mask)):\n",
    "            # if galaxy is inside path, unmask it (set mask=0 for that galaxy)\n",
    "            if path.contains_point(gxy_pos_transform[j]):\n",
    "                #if MBH_fbulge1[j] > 0.0: #only keep galaxy if not negative mass (quasar)\n",
    "                mask[j] = 0\n",
    "\n",
    "    # which galaxies are inside the contour?\n",
    "    w_in = np.where(mask == 0)[0]\n",
    "    print('Galaxies in region:', len(w_in))\n",
    "\n",
    "    # plot galaxies in localization region\n",
    "    ax.plot(np.ma.masked_array(gxy_pos_transform[:,0], mask=mask),\n",
    "            np.ma.masked_array(gxy_pos_transform[:,1], mask=mask),\n",
    "            color='turquoise', markeredgecolor='k', marker='o',\n",
    "            ls='', alpha=0.7, ms=3)\n",
    "\n",
    "    # plot galaxies passing distance cut\n",
    "    if second_mask is not None:\n",
    "        ax.plot(np.ma.masked_array(gxy_pos_transform[:,0], mask=second_mask),\n",
    "                np.ma.masked_array(gxy_pos_transform[:,1], mask=second_mask),\n",
    "                color='gold', markeredgecolor='k', marker='o',\n",
    "                ls='', alpha=0.7, ms=3)\n",
    "\n",
    "    # zoom-in plotting\n",
    "    if zoom:\n",
    "        \n",
    "        max_ra = max(host_ra[w_in])\n",
    "        min_ra = min(host_ra[w_in])\n",
    "        max_dec = max(host_dec[w_in])\n",
    "        min_dec = min(host_dec[w_in])\n",
    "        \n",
    "        rad = str(max([max_ra-min_ra, max_dec-min_dec])-10) + ' deg'\n",
    "        center = str((max_ra+min_ra)/2) + 'd ' + str((max_dec+min_dec)/2) + 'd'\n",
    "\n",
    "        ax_zoom_rect = plt.axes([1.0, 0.3, 0.4, 0.4],\n",
    "                                projection='astro degrees zoom',\n",
    "                                center=center,\n",
    "                                radius=rad)\n",
    "        ax_zoom_rect.imshow_hpx(skymap, cmap='Oranges', smooth=None)\n",
    "        zoomContour = ax_zoom_rect.contour_hpx(skymap, cmap=mpl.colors.ListedColormap(['k']), linewidths=1.5,\n",
    "                                               levels=[skymap[w_cl][cut_cl]])\n",
    "        ax_zoom_rect.set_xlabel(' ')\n",
    "        ax_zoom_rect.set_ylabel(' ')\n",
    "        \n",
    "        tzoom_obj = ax_zoom_rect.get_transform('world') + ax_zoom_rect.transData.inverted()\n",
    "        gxy_pos_tzoom = tzoom_obj.transform(gxy_pos)\n",
    "        mask_zoom = np.ones(43532, dtype=bool)\n",
    "        for path in zoomContour.collections[0].get_paths():\n",
    "            for j in range(len(mask_zoom)):\n",
    "                if path.contains_point(gxy_pos_tzoom[j]):\n",
    "                    mask_zoom[j] = 0\n",
    "\n",
    "        ax_zoom_rect.plot(np.ma.masked_array(gxy_pos_tzoom[:,0], mask=mask_zoom),\n",
    "                          np.ma.masked_array(gxy_pos_tzoom[:,1], mask=mask_zoom),\n",
    "                          color='turquoise', markeredgecolor='k', marker='o',\n",
    "                          ls='', alpha=0.7, ms=3)\n",
    "\n",
    "        if second_mask is not None:\n",
    "            ax_zoom_rect.plot(np.ma.masked_array(gxy_pos_tzoom[:,0], mask=second_mask),\n",
    "                              np.ma.masked_array(gxy_pos_tzoom[:,1], mask=second_mask),\n",
    "                              color='gold', markeredgecolor='k', marker='o',\n",
    "                              ls='', alpha=0.7, ms=3)\n",
    "\n",
    "        ax_zoom_rect.grid(alpha=0.3, color='k')\n",
    "        ax_zoom_rect.tick_params(axis='both', which='major')\n",
    "\n",
    "        ax.mark_inset_axes(ax_zoom_rect)\n",
    "        ax.connect_inset_axes(ax_zoom_rect, 'upper right')\n",
    "        ax.connect_inset_axes(ax_zoom_rect, 'lower right')\n",
    "    \n",
    "    # other plotting params\n",
    "    ax.grid(alpha=0.3, color='k')\n",
    "    ax.tick_params(axis='both', which='major')\n",
    "    plt.show()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3c0cf-5de6-46fa-8993-dc475fc8e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_mask = plot_hosts(gxy_ra, gxy_dec, norm_map, cl=0.9, zoom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf84bd-4748-4b21-b91c-5306a6d46edb",
   "metadata": {},
   "source": [
    "So we've eliminated 42,161 galaxies (~97%) so far by looking only at those that lie in our localization area. But don't be fooled by percentages $-$ 1371 is still a large number of galaxies to sift through! Maybe we can use some other host properties we can use to discard more potential hosts..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63828da6-4ee7-4efa-b3e0-b823ab203361",
   "metadata": {},
   "source": [
    "# 5. Process of host elimination\n",
    "\n",
    "***WARNING: You are now entering exploratory territory.*** How do we find the true host galaxy, or at least narrow down the number of potential host galaxies, starting from 1000+ candidates? This question is in the process of being explored, but it's a very important one to answer if we want to achieve a coordinated multi-messenger detection of a SMBHB system! In this section, we'll just begin to scratch the surface when it comes to figuring out how to cut out more hosts.\n",
    "\n",
    "As is, our galaxy catalog contains **distance** and **SMBH mass** estimates for each of our galaxies. For now, let's use these two pieces of information to see if we can do better to eliminate additional host candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd714cb-74a6-4a0b-82d2-f2062136da57",
   "metadata": {},
   "source": [
    "### Implementing a distance cut\n",
    "One of the simplest things we can do is to look at the entire three-dimensional localization **volume** as opposed to just the localization area on the sky. To do this, we'll need to convert our posterior samples on the GW frequency, strain, and chirp mass to posterior samples on the luminosity distance. We can do this by rearranging the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_{0} = \\frac{2 \\mathcal{M}^{5/3}(\\pi f_{\\rm{GW}})^{2/3}}{d_{L}}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761708e-60a9-4709-8520-6d52eef41265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the relevant samples from our chain\n",
    "fgw_samples = 10**chain[burn:,3]\n",
    "h_samples = 10**chain[burn:,4]\n",
    "mc_samples = 10**chain[burn:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b231a72-e45f-47ae-82e1-cead9b633ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into samples on the luminosity distance\n",
    "# we'll use log-distance samples going forward, since we log-sampled in frequency, strain, and chirp mass\n",
    "dist_samples = (2 * (mc_samples*const.Tsun)**(5/3) * (np.pi * fgw_samples)**(2/3) / h_samples) / const.Mpc * const.c\n",
    "logdist_samples = np.log10(dist_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05fa57-9daa-48c9-a584-84b29f3bfa1f",
   "metadata": {},
   "source": [
    "For the simple distance cut we'll do here, we're just going to choose one or two cutoff values based on our luminosity distance posterior. To be a little conservative, let's take the 95% credible interval (where 95% of the posterior samples fall in the distribution) and cut out any galaxies that have distances lying beyond the edges of this interval. To visualize this, run the next couple cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633ab75-40c6-44f8-b743-066e0dc1ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the 95% credible interval, we need to find the distance values corresponding to the 50 +/- (95/2) percentiles\n",
    "perc_cuts = [2.5, 97.5]\n",
    "cutoffs = [np.percentile(logdist_samples, q=perc) for perc in perc_cuts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfb09a-a196-4e81-ac6c-a2e06447e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the luminosity distance posterior distribution\n",
    "plt.hist(logdist_samples, bins=50, histtype='step', lw=2, density=True)\n",
    "\n",
    "# plot the cutoff values for comparison\n",
    "for cut in cutoffs:\n",
    "    plt.axvline(cut, ls='--', color='r')\n",
    "\n",
    "plt.xlabel('$\\log_{10}d_L$ posterior samples')\n",
    "plt.xlim(0,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beccf4b-bb7d-4099-90e6-d43911ad3805",
   "metadata": {},
   "source": [
    "Now use the `dist_cut` function below to perform the cut and see how many galaxies remain afterwards. We'll need a list of galaxy distances (for ALL galaxies in the catalog), the luminosity distance posterior samples, our cutoff values, and our sky mask that we made earlier. The function will then make a new mask that will mask out galaxies now according to **two criteria**: whether or not it lies in the localization area, and whether or not it falls within the distance cut boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20423d6b-e9a2-427a-8807-126673a77f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_cut(gxy_dists, logd_samples, cutoffs, mask):\n",
    "    \n",
    "    '''\n",
    "    gxy_dists: array of host galaxy distances\n",
    "    logd_samples: log luminosity distance posterior samples\n",
    "    cutoffs: value(s) with which to perform distance cut(s). can be one or two values. if two, should be in ascending order.\n",
    "    mask: mask of galaxies lying within localization area (0=included, 1=excluded)\n",
    "    '''\n",
    "\n",
    "    # make copy of sky mask for new distance mask\n",
    "    dmask = np.copy(mask)\n",
    "    \n",
    "    # if using 2 cuts\n",
    "    if len(cutoffs) > 1:\n",
    "        # for each galaxy, first check if the galaxy is not already masked out\n",
    "        for i,d in enumerate(np.log10(gxy_dists)):\n",
    "            if dmask[i] != 1:\n",
    "                # mask out the galaxy if it lies outside the boundaries\n",
    "                if d < cutoffs[0] or d > cutoffs[1]:\n",
    "                    dmask[i] = 1\n",
    "\n",
    "    # if using 1 cut\n",
    "    else:\n",
    "        # check whether cutoff value is below or above the median to determine the type of cut (lower/upper)\n",
    "        p50 = np.percentile(logd_samples, q=50)\n",
    "        print(p50)\n",
    "\n",
    "        # lower cut\n",
    "        if cutoffs[0] < p50:\n",
    "            # for each galaxy, first check if the galaxy is not already masked out\n",
    "            for i,d in enumerate(np.log10(gxy_dists)):\n",
    "                if dmask[i] != 1:\n",
    "                    # mask out the galaxy if it lies below the boundary\n",
    "                    if d < cutoffs[0]:\n",
    "                        dmask[i] = 1\n",
    "            \n",
    "        # upper cut\n",
    "        else:\n",
    "            # for each galaxy, first check if the galaxy is not already masked out\n",
    "            print(len(np.where(dmask==0)[0]))\n",
    "            for i,d in enumerate(np.log10(gxy_dists)):\n",
    "                if dmask[i] != 1:\n",
    "                    # mask out the galaxy if it lies above the boundary\n",
    "                    if d > cutoffs[0]:\n",
    "                        dmask[i] = 1\n",
    "\n",
    "    return dmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766bdbf-1e5d-4a8a-a14d-8312484c87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sky + distance mask\n",
    "sky_dist_mask = dist_cut(dists, logdist_samples, cutoffs, sky_mask)\n",
    "\n",
    "# find which galaxies are not masked out and count them\n",
    "w_in_d = np.where(sky_dist_mask == 0)[0]\n",
    "print('Galaxies remaining after distance cut:', len(w_in_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46bb68-a003-469c-8470-7cdbdf41a2e3",
   "metadata": {},
   "source": [
    "Let's visualize this. We can replot our skymap with all the galaxies in the localization area in turquoise, and those that pass the distance cut in gold. To do this, supply the `second_mask` argument in the `plot_hosts` function with the sky + distance mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03d113-28d0-4e88-bca9-df714eb9daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with distance mask added on top\n",
    "_ = plot_hosts(gxy_ra, gxy_dec, norm_map, cl=0.9, zoom=True, second_mask=sky_dist_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e5e6a-7d5a-4108-bade-bcd334e71587",
   "metadata": {},
   "source": [
    "So a 95% credible interval distance cut helps us a little bit. In this case, we went from 1371 potential hosts down to 1037 potential hosts, cutting out ~300 galaxies.\n",
    "\n",
    "**Let's go back and try some different cutoff values**, i.e., change the values in the `perc_cuts` array. What happens if you set your boundaries closer and closer to the peak of the luminosity distance posterior distribution? e.g., Change this to a **68% credible interval cut (about ~1$\\sigma$), or a 50% credible interval cut**, and see how many galaxies you're left with. **What about a single-value cut, only the lower 10%? Upper 10%?** Try any other values you're interested in investigating!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89097b6d-e91d-41db-89d0-450f7df741ec",
   "metadata": {},
   "source": [
    "### Implementing a SMBH mass cut\n",
    "\n",
    "The other piece of information we have in the galaxy catalog is a SMBH mass estimate $-$ we can assume that this is the total binary mass, i.e., assume that the galaxy contains a binary, and the quoted SMBH mass is the mass of both SMBHs combined. However, when analyzing the PTA data, we sample over the chirp mass of the binary, which is related to the total mass by the following relation: $\\mathcal{M} = q^{3/5}/(1+q)^{6/5}M_{\\mathrm{tot}}$\n",
    "\n",
    "with $\\mathcal{M}$ being the chirp mass and $M_{\\mathrm{tot}}$ being the total binary mass taken from the galaxy catalog. The $q$ in this equation represents the mass ratio of the binary, which we unfortunately don't have any information about from our GW search. So we have a few options in terms of converting our chirp mass posterior samples into total mass samples:\n",
    "<il>\n",
    "- Make a reasonable assumption about the mass ratio. We expect PTA sources to mostly come from major mergers, i.e., $q \\geq 0.25$.\n",
    "- Draw from a distribution of mass ratio values. For example, between $0.25 \\leq q \\leq 1$, or if we want to be more conservative, between $0.1 \\leq q \\leq 1$.\n",
    "- Use rejection sampling to decompose our chirp mass samples into samples on the mass ratio and total binary mass.\n",
    "</il>\n",
    "\n",
    "We'll go with option 1 for now, and just assume an equal-mass binary in each case, i.e., $q = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b06d5-1941-4a64-ab1b-5e1a567a0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert chirp mass samples to total mass samples\n",
    "q = 1.\n",
    "mtot_samples = mc_samples / (q**(3/5) / (1+q)**(6/5))\n",
    "\n",
    "# plot histogram of total mass samples\n",
    "plt.hist(np.log10(mc_samples), bins=50, histtype='step', lw=2, label='chirp mass', density=True)\n",
    "plt.hist(np.log10(mtot_samples), bins=50, histtype='step', lw=2, label='total mass', density=True)\n",
    "plt.xlabel('log mass posterior')\n",
    "plt.xlim(8,11)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a54ca3-6376-424b-b5b8-4cad1977848d",
   "metadata": {},
   "source": [
    "**Let's implement a similar kind of cut here and compare how well we do with our results from the distance cut.** The function below is exactly like the distance cut function, instead taking the galaxies' SMBH mass estimates, the total binary mass posterior samples, and new cutoff values. **How well can you do with a 95% credible interval cut?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e3b65-e8a2-4327-8016-ca5fabf6ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBH_cut(gxy_MBHs, logmtot_samples, cutoffs, mask):\n",
    "    \n",
    "    '''\n",
    "    gxy_MBHs: array of host galaxy SMBH mass estimates\n",
    "    logmtot_samples: log total binary mass posterior samples\n",
    "    cutoffs: value(s) with which to perform distance cut(s). can be one or two values. if two, should be in ascending order.\n",
    "    mask: mask of galaxies lying within localization area (0=included, 1=excluded)\n",
    "    '''\n",
    "\n",
    "    # make copy of sky mask for new distance mask\n",
    "    mmask = np.copy(mask)\n",
    "    \n",
    "    # if using 2 cuts\n",
    "    if len(cutoffs) > 1:\n",
    "        # for each galaxy, first check if the galaxy is not already masked out\n",
    "        for i,m in enumerate(gxy_MBHs):\n",
    "            if mmask[i] != 1:\n",
    "                # mask out the galaxy if it lies outside the boundaries\n",
    "                if m < cutoffs[0] or m > cutoffs[1]:\n",
    "                    mmask[i] = 1\n",
    "\n",
    "    # if using 1 cut\n",
    "    else:\n",
    "        # check whether cutoff value is below or above the median to determine the type of cut (lower/upper)\n",
    "        p50 = np.percentile(logmtot_samples, q=50)\n",
    "        print(p50)\n",
    "\n",
    "        # lower cut\n",
    "        if cutoffs[0] < p50:\n",
    "            # for each galaxy, first check if the galaxy is not already masked out\n",
    "            for i,d in enumerate(gxy_MBHs):\n",
    "                if mmask[i] != 1:\n",
    "                    # mask out the galaxy if it lies below the boundary\n",
    "                    if m < cutoffs[0]:\n",
    "                        mmask[i] = 1\n",
    "            \n",
    "        # upper cut\n",
    "        else:\n",
    "            # for each galaxy, first check if the galaxy is not already masked out\n",
    "            print(len(np.where(mmask==0)[0]))\n",
    "            for i,d in enumerate(gxy_MBHs):\n",
    "                if mmask[i] != 1:\n",
    "                    # mask out the galaxy if it lies above the boundary\n",
    "                    if m > cutoffs[0]:\n",
    "                        mmask[i] = 1\n",
    "\n",
    "    return mmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5febf50-14aa-4c33-bdf2-3f68b3730ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose your cuts\n",
    "perc_cuts = [__]\n",
    "m_cutoffs = [np.percentile(np.log10(mtot_samples), q=perc) for perc in perc_cuts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ae452-bf56-45f9-a8af-ce24f81402e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the total binary mass posterior distribution with the cutoffs\n",
    "\n",
    "\n",
    "# plot the cutoff values for comparison\n",
    "\n",
    "\n",
    "plt.xlabel(r'$\\log_{10}M_{\\rm{tot}}$ posterior samples')\n",
    "plt.xlim(8,11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769c3a7-8902-4484-ba05-c936a839c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sky + BH mass mask\n",
    "sky_MBH_mask = MBH_cut(BHmasses, np.log10(mtot_samples), m_cutoffs, sky_mask)\n",
    "\n",
    "# find which galaxies are not masked out and count them\n",
    "w_in_m = np.where(sky_MBH_mask == 0)[0]\n",
    "print('Galaxies remaining after BH mass cut:', len(w_in_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0ae6c-344a-4c5b-9862-18cb581bb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with BH mass mask added on top\n",
    "_ = plot_hosts(gxy_ra, gxy_dec, norm_map, cl=0.9, zoom=True, second_mask=sky_MBH_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7eeb0-3abb-4066-9002-1bd0db65bfde",
   "metadata": {},
   "source": [
    "**Did your BH mass cut perform better than the distance cut? Can your BH mass cut do even better with more stringent cutoff values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88938693-4534-4734-b2a9-7b14b39ed902",
   "metadata": {},
   "source": [
    "### True galaxy check\n",
    "\n",
    "Narrowing down the number of hosts is a crucial task, but we need to be careful, as we don't want to cut out the true galaxy hosting the SMBHB that made our GW signal by mistake! In the section below, I'll give you the galaxy ID for the true galaxy into which the CW signal was injected. **Use your sky+distance mask and your sky+mass mask to check if the true galaxy is among those remaining after your cuts. Is the true galaxy among those remaining after your distance cut? How about your mass cut?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674f07a-765a-46e6-89c8-86b7ff4a21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sky coordinates for the true galaxy\n",
    "true_ID = '13000809+2758372'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb68e1-5f97-465d-897e-d9cc6c4f3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check galaxies remaining after distance cut\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede9389-7a5f-4bff-b539-7fadd147a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check galaxies remaining after BH mass cut\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de68a7-29cc-4750-a56e-f73f500f658f",
   "metadata": {},
   "source": [
    "# Bonus material!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca95dd0-85e1-4524-adf5-49955ed8c4cd",
   "metadata": {},
   "source": [
    "### Different BH mass cuts\n",
    "\n",
    "Use the space below to try out different forms of BH mass cuts mentioned earlier. Some ideas:\n",
    "<il>\n",
    "- Maybe assuming equal-mass binaries with mass ratios of $q=1$ is a little idealistic... try using different mass ratio assumptions.\n",
    "- For each chirp mass posterior sample, draw a $q$ value from a distribution, e.g., use values corresponding to major mergers. Calculate the corresponding total mass and create your total mass distribution that way.\n",
    "- Try a different type of cut entirely! For each BH mass estimate in our galaxy catalog, we have uncertainties on the BH masses as well. You can assume a normal distribution for each galaxy's BH mass, and then calculate the overlap between this distribution and the total mass posterior.\n",
    "</il>\n",
    "\n",
    "**Minor hint:** The injected CW signal had a mass ratio of $q=0.005$. In reality, though, we don't expect to see binaries with such low mass ratios... this signal was just for a test run and wasn't meant to be completely realistic. But using that information, try your BH mass cut again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a584707-5825-4cde-8819-08b61a5295ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for BH mass cut exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656d548-b067-42d1-8db0-686620fcc18f",
   "metadata": {},
   "source": [
    "### Combo distance + BH mass cut\n",
    "\n",
    "**Note:** if you altered the way the total binary mass posterior was calculated, you may need to change the x-limits in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6ba12-4042-42c6-96a6-9eca5deced85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both posteriors and all galaxies\n",
    "plt.hist2d(np.log10(mtot_samples), logdist_samples, bins=100)\n",
    "plt.scatter(BHmasses, np.log10(dists), marker='.', color='w', alpha=0.1, s=3)\n",
    "plt.xlabel('log10_Mtot')\n",
    "plt.ylabel('log10_dL')\n",
    "plt.xlim(7.5,9.9)\n",
    "plt.ylim(0,3.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817bb19-af8c-47b6-9c67-03aa4a018470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for dist + BH mass cut exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad4f2a-9070-4494-80e8-8ab8516fc2a5",
   "metadata": {},
   "source": [
    "### Cuts with original galaxy data\n",
    "\n",
    "If you're feeling brave, you can also revisit the original survey data from which our galaxy catalog was made, i.e., the 2MASS Redshift Survey data. **Check out the README file in the 2MRS directory of this repo to see what kind of data we have for each galaxy.** Things that might be helpful for host galaxy identification, for example, include magnitudes in different infrared bands ($K$-, $H$-, and $J$-band magnitudes) and morphological classifications. The galaxy IDs will be the same as those in the catalog we've worked with so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ca52a-9e45-4fe3-b4c0-c17b303e50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fb4b0-317c-42c3-bc68-83f413dfd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in survey file\n",
    "survey_file = basedir+'/2MRS/2mrs_1175_done.fits'\n",
    "survey_data = fits.open(survey_file)[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b83e3-ca65-40fe-8a8c-7014449e3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab some important values\n",
    "ID = []\n",
    "K_tc = []\n",
    "e_Kt = []\n",
    "ext_BV = []\n",
    "types = []\n",
    "for i in range(0,len(survey_data)-1):\n",
    "    ID.append(survey_data[i][0])\n",
    "    K_tc.append(survey_data[i][8])\n",
    "    e_Kt.append(survey_data[i][14])\n",
    "    ext_BV.append(survey_data[i][17])\n",
    "    types.append(survey_data[i][22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c619c-5b12-49f7-905f-ac237569847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute K-band magnitude calculations\n",
    "R_V = 3.1 #reddening relation\n",
    "K_abs = []\n",
    "for i in range(0,len(K_tc)):\n",
    "    m_K = K_tc[i]\n",
    "    E_BV = ext_BV[i]\n",
    "    A_V = R_V*E_BV #A_V is extragalactic extinction\n",
    "    M_K = m_K-5*np.log10(dists[i])-25-0.11*A_V\n",
    "    K_abs.append(M_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee06bcc-c300-4eb7-aa5f-1e974b31cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total stellar mass of the galaxy\n",
    "M_stel = [10**(10.58-0.44*(abs_mag+23)) for abs_mag in K_abs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78417252-e681-4dab-89af-ced3553410e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space to explore other cuts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d07cbf-d438-4afa-b864-214af9f99d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vss",
   "language": "python",
   "name": "vss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
